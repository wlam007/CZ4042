{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 entropy 2.3002524\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-448cec033397>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m   \u001b[0mq1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m   \u001b[0;31m#q2()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m   \u001b[0;31m#q3(50, 60) #opitmal c1 feature map, optimal c2 feature map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-448cec033397>\u001b[0m in \u001b[0;36mq1\u001b[0;34m()\u001b[0m\n\u001b[1;32m    378\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpool_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpool_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m     \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0mprintCombinedGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloss_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"test accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"accuracy / cost\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"./drive/My Drive/Colab Notebooks/q1_accuracies.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-448cec033397>\u001b[0m in \u001b[0;36mbatch_train\u001b[0;34m(print_pattern)\u001b[0m\n\u001b[1;32m    215\u001b[0m               \u001b[0mtest_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtestX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtestY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m               \u001b[0mloss_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m               \u001b[0mtrain_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m               \u001b[0mtest_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtestX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtestY\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m    693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m     \"\"\"\n\u001b[0;32m--> 695\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[0;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   5179\u001b[0m                        \u001b[0;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5180\u001b[0m                        \"graph.\")\n\u001b[0;32m-> 5181\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#\n",
    "# Project 2, starter code Part a\n",
    "#\n",
    "\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "import pickle\n",
    "import pylab\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "import sklearn.model_selection as sk\n",
    "\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "\n",
    "\n",
    "\n",
    "NUM_CLASSES = 10\n",
    "IMG_SIZE = 32\n",
    "NUM_CHANNELS = 3\n",
    "learning_rate = 0.001\n",
    "epochs = 100\n",
    "batch_size = 128\n",
    "\n",
    "c1_num_feature_map = 50\n",
    "c2_num_feature_map = 60\n",
    "\n",
    "feature_map_start_1 = 50\n",
    "feature_map_increment_1 = 10\n",
    "feature_map_end_1 = 111\n",
    "\n",
    "feature_map_start = 50\n",
    "feature_map_increment = 10\n",
    "feature_map_end = 111\n",
    "\n",
    "print_interval= 10\n",
    "\n",
    "is_momentum = False\n",
    "moumentum = 0.1\n",
    "\n",
    "is_rmsprop_learning_algo = False\n",
    "is_adam_optimizer = False\n",
    "is_dropout_layer = False\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = None\n",
    "keep = 0.8\n",
    "\n",
    "seed = 10\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "\n",
    "trainX = None\n",
    "trainY = None\n",
    "testX = None\n",
    "testY = None\n",
    "x = None\n",
    "y_ = None\n",
    "logits = None\n",
    "loss = None\n",
    "train_step = None\n",
    "W1 = None\n",
    "b1 = None\n",
    "conv_1 = None\n",
    "pool_1 = None \n",
    "W2 = None\n",
    "b2 = None\n",
    "conv_2  = None\n",
    "pool_2 = None \n",
    "W3 = None \n",
    "b3 = None\n",
    "accuracy = None\n",
    "\n",
    "def load_data(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        try:\n",
    "            samples = pickle.load(fo)\n",
    "        except UnicodeDecodeError:  #python 3.x\n",
    "            fo.seek(0)\n",
    "            samples = pickle.load(fo, encoding='latin1')\n",
    "\n",
    "    data, labels = samples['data'], samples['labels']\n",
    "\n",
    "    data = np.array(data, dtype=np.float32)\n",
    "    labels = np.array(labels, dtype=np.int32)\n",
    "\n",
    "    \n",
    "    labels_ = np.zeros([labels.shape[0], NUM_CLASSES])\n",
    "    labels_[np.arange(labels.shape[0]), labels-1] = 1\n",
    "\n",
    "    return data, labels_\n",
    "  \n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def cnn(images):\n",
    "    global keep_prob, h_fc1_drop\n",
    "    images = tf.reshape(images, [-1, IMG_SIZE, IMG_SIZE, NUM_CHANNELS])\n",
    "    \n",
    "    #Conv 1\n",
    "    W1 = tf.Variable(tf.truncated_normal([9, 9, NUM_CHANNELS, c1_num_feature_map], stddev=1.0/np.sqrt(NUM_CHANNELS*9*9)), name='weights_1')\n",
    "    b1 = tf.Variable(tf.zeros([c1_num_feature_map]), name='biases_1')\n",
    "    conv_1 = tf.nn.relu(tf.nn.conv2d(images, W1, [1, 1, 1, 1], padding='VALID') + b1)\n",
    "    \n",
    "    #pool 1\n",
    "    pool_1 = tf.nn.max_pool(conv_1, ksize= [1, 2, 2, 1], strides= [1, 2, 2, 1], padding='VALID', name='pool_1')\n",
    "    #dim = pool_1.get_shape()[1].value * pool_1.get_shape()[2].value * pool_1.get_shape()[3].value \n",
    "    \n",
    "    #Conv 2\n",
    "    W2 = tf.Variable(tf.truncated_normal([5, 5, c1_num_feature_map, c2_num_feature_map], stddev=1.0/np.sqrt(c1_num_feature_map*5*5)), name='weights_2')\n",
    "    b2 = tf.Variable(tf.zeros([c2_num_feature_map]), name='biases_2')\n",
    "    conv_2 = tf.nn.relu(tf.nn.conv2d(pool_1, W2, [1, 1, 1, 1], padding='VALID') + b2)\n",
    "    \n",
    "    #pool 2\n",
    "    pool_2 = tf.nn.max_pool(conv_2, ksize= [1, 2, 2, 1], strides= [1, 2, 2, 1], padding='VALID', name='pool_2')\n",
    "    dim = pool_2.get_shape()[1].value * pool_2.get_shape()[2].value * pool_2.get_shape()[3].value \n",
    "    #pool_2_flat = tf.reshape(pool_2, [-1, dim_1])\n",
    "    \n",
    "    #Fully connected layer 1 of size 300\n",
    "    W_fc1 = tf.Variable(tf.truncated_normal([dim, 300], stddev=1.0/np.sqrt(dim)), name='weights_fc1')\n",
    "    b_fc1 = tf.Variable(tf.zeros([300]), name='biases_fc1')\n",
    "    h_pool2_flat = tf.reshape(pool_2, [-1, dim])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "    dim_1 = 300\n",
    "    \n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "    \n",
    "    #Softmax\n",
    "    W3, b3, logits = None, None, None\n",
    "    \n",
    "    if is_dropout_layer:\n",
    "      #Softmax with dropout\n",
    "      W3 = tf.Variable(tf.truncated_normal([dim_1, NUM_CLASSES], stddev=1.0/np.sqrt(dim_1)), name='weights_3')\n",
    "      b3 = tf.Variable(tf.zeros([NUM_CLASSES]), name='biases_3')\n",
    "      logits = tf.matmul(h_fc1_drop, W3) + b3\n",
    "    else:\n",
    "      #Softmax without dropout\n",
    "      W3 = tf.Variable(tf.truncated_normal([dim_1, NUM_CLASSES], stddev=1.0/np.sqrt(dim_1)), name='weights_3')\n",
    "      b3 = tf.Variable(tf.zeros([NUM_CLASSES]), name='biases_3')\n",
    "      logits = tf.matmul(h_fc1, W3) + b3\n",
    "\n",
    "    return W1, b1, conv_1, pool_1, W2, b2, conv_2, pool_2, W3, b3, logits\n",
    "\n",
    "\n",
    "def main():\n",
    "    global accuracy, trainX, trainY, testX, testY, x, y_, logits, loss, train_step, W1, b1, conv_1, pool_1, W2, b2, conv_2, pool_2, W3, b3\n",
    "    \n",
    "    trainX, trainY = load_data('./drive/My Drive/Colab Notebooks/data_batch_1')\n",
    "    #print(trainX.shape, trainY.shape)\n",
    "    \n",
    "    testX, testY = load_data('./drive/My Drive/Colab Notebooks/test_batch_trim')\n",
    "    #print(testX.shape, testY.shape)\n",
    "\n",
    "    testX = (testX - np.min(testX, axis = 0))/np.max(testX, axis = 0)\n",
    "\n",
    "    trainX = (trainX - np.min(trainX, axis = 0))/np.max(trainX, axis = 0)\n",
    "\n",
    "    # Create the model\n",
    "    x = tf.placeholder(tf.float32, [None, IMG_SIZE*IMG_SIZE*NUM_CHANNELS])\n",
    "    y_ = tf.placeholder(tf.float32, [None, NUM_CLASSES])\n",
    "\n",
    "    \n",
    "    W1, b1, conv_1, pool_1, W2, b2, conv_2, pool_2, W3, b3, logits = cnn(x)\n",
    "\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_, logits=logits)\n",
    "    loss = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "    train_step = None\n",
    "    if is_momentum:\n",
    "      train_step = tf.train.MomentumOptimizer(learning_rate, moumentum).minimize(loss)\n",
    "      \n",
    "    elif is_rmsprop_learning_algo:\n",
    "      train_step = tf.train.RMSPropOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "    elif is_adam_optimizer:\n",
    "      train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "    else:\n",
    "      #default\n",
    "      train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y_, 1))\n",
    "    correct_prediction = tf.cast(correct_prediction, tf.float32)\n",
    "    accuracy = tf.reduce_mean(correct_prediction)\n",
    "    return \n",
    "   \n",
    "\n",
    "def batch_train(print_pattern = False):\n",
    "    global accuracy, trainX, trainY, testX, testY, x, y_, logits, loss, train_step, W1, b1, conv_1, pool_1, W2, b2, conv_2, pool_2, W3, b3\n",
    "\n",
    "    main() \n",
    "    N = len(trainX)\n",
    "    idx = np.arange(N)\n",
    "    loss_acc = []\n",
    "    train_acc = []\n",
    "    test_acc = []\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        for e in range(epochs):\n",
    "            np.random.shuffle(idx)\n",
    "            trainX, trainY = trainX[idx], trainY[idx]\n",
    "\n",
    "            #batch train\n",
    "            for start, end in zip(range(0, N, batch_size), range(batch_size, N, batch_size)):\n",
    "                if is_dropout_layer:\n",
    "                  train_step.run(feed_dict={x: trainX[start: end], y_: trainY[start: end], keep_prob: keep})\n",
    "                else:\n",
    "                  train_step.run(feed_dict={x: trainX[start: end], y_: trainY[start: end]})\n",
    "\n",
    "            loss_, train_accuracy, test_accuracy = None, None, None\n",
    "           \n",
    "            if is_dropout_layer:\n",
    "              loss_ = loss.eval(feed_dict= {x: trainX, y_ : trainY, keep_prob: 1.0})\n",
    "              train_accuracy = accuracy.eval(feed_dict= {x: trainX, y_ : trainY, keep_prob: 1.0})\n",
    "              test_accuracy = accuracy.eval(feed_dict= {x: testX, y_ : testY, keep_prob: 1.0})\n",
    "            else:\n",
    "              loss_ = loss.eval(feed_dict= {x: trainX, y_ : trainY})\n",
    "              train_accuracy = accuracy.eval(feed_dict= {x: trainX, y_ : trainY})\n",
    "              test_accuracy = accuracy.eval(feed_dict= {x: testX, y_ : testY})\n",
    "                \n",
    "            train_acc.append(train_accuracy)\n",
    "            test_acc.append(test_accuracy)\n",
    "            loss_acc.append(loss_)\n",
    "\n",
    "            if print_pattern:\n",
    "              if e % print_interval == 0:\n",
    "                  print('epoch', e, 'entropy', loss_)\n",
    "\n",
    "              if e > epochs -2:\n",
    "                print(\"test pattern 1\")\n",
    "                plotTestPattern(sess, \"1-\")\n",
    "\n",
    "                print(\"test pattern 2\")\n",
    "                plotTestPattern(sess,  \"2-\")\n",
    "\n",
    "        return train_acc, test_acc, loss_acc\n",
    "      \n",
    "def cv_train():\n",
    "    global accuracy, trainX, trainY, testX, testY, x, y_, logits, loss, train_step, W1, b1, conv_1, pool_1, W2, b2, conv_2, pool_2, W3, b3\n",
    "\n",
    "    #init graph\n",
    "    main()\n",
    "    N = len(trainX)\n",
    "    idx = np.arange(N)\n",
    "    np.random.shuffle(idx)\n",
    "    trainX, trainY = trainX[idx], trainY[idx]\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        #init dataset used for training\n",
    "        train_acc = []\n",
    "        val_acc = []\n",
    "        test_acc = []\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            #each epoch will run 5-fold cross validation training\n",
    "\n",
    "            kfold = KFold(n_splits=5) #k-fold split\n",
    "            fold = 0\n",
    "            train_acc = []\n",
    "            for idx, val_x in kfold.split(trainX):\n",
    "                #split data into CV sets\n",
    "                train_X = trainX[idx]\n",
    "                train_Y = trainY[idx]\n",
    "                val_X = trainX[val_x]\n",
    "                val_Y = trainY[val_x]\n",
    "\n",
    "                #shuffle data in the 4 train fold\n",
    "                N1 = len(train_X)\n",
    "                idx1 = np.arange(N1)\n",
    "                np.random.shuffle(idx1)\n",
    "                train_x, train_y = train_X[idx1], train_Y[idx1]\n",
    "                \n",
    "                #for each Cross Val set, run train epochs in batches\n",
    "                for start, end in zip(range(0, N, batch_size), range(batch_size, N, batch_size)):\n",
    "                  train_step.run(feed_dict={x: train_x[start: end], y_: train_y[start: end]})\n",
    "                    \n",
    "                #after each fold's training, check val accuracy\n",
    "                train_acc.append(accuracy.eval(feed_dict={x: val_X, y_: val_Y}))\n",
    "                \n",
    "                #if i % print_interval == 0:\n",
    "                #    print('iter %d: validation accuracy for fold %d %g'%(i, fold, train_acc[fold]))\n",
    "                    \n",
    "                fold +=1\n",
    "            \n",
    "            val_accuracy = sum(train_acc)/5\n",
    "            #if i % print_interval == 0:\n",
    "            #    print('iter %d: validation accuracy %g'%(i, val_accuracy) )\n",
    "            val_acc.append( val_accuracy )\n",
    "            \n",
    "            test_accuracy = accuracy.eval(feed_dict= {x: testX, y_ : testY})\n",
    "            test_acc.append(test_accuracy)\n",
    "        print(val_acc)\n",
    "        print(test_acc)\n",
    "        return val_acc, test_acc\n",
    "    \n",
    "  \n",
    "def printCombinedGraph(accuracies, epoch, hyperparam, y_label, label, saveAs = \"./drive/My Drive/Colab Notebooks/def_combined.png\"):\n",
    "    n = len(hyperparam)\n",
    "\n",
    "    fig, train_accuracy = plt.subplots(nrows=1, ncols=1) \n",
    "    for i in range(n):\n",
    "        train_accuracy.plot(range(epoch), accuracies[i], label='{} = {}'.format(label, hyperparam[i]))\n",
    "        \n",
    "\n",
    "    plt.xlabel('epoch')\n",
    "    train_accuracy.set_ylabel(y_label)\n",
    "    train_accuracy.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.draw()\n",
    "    plt.gcf().subplots_adjust(bottom=0.15)\n",
    "    plt.savefig(saveAs, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    \n",
    "def printSingleGraph(x, y, xlabel=\"\", ylabel =\"\", saveAs = \"single.png\"):\n",
    "     #plot invidual curve\n",
    "    plt.figure(1)\n",
    "    plt.plot(range(y), x)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.draw()\n",
    "    plt.gcf().subplots_adjust(bottom=0.15)\n",
    "    plt.show()\n",
    "    plt.savefig(saveAs, bbox_inches=\"tight\")\n",
    "\n",
    "def plotTestPattern(sess, key= \"\", X = -1):\n",
    "    #for qn1 part b\n",
    "    global trainX, trainY, testX, testY, x, y_, logits, loss, train_step, W1, b1, conv_1, pool_1, W2, b2, conv_2, pool_2, W3, b3\n",
    "\n",
    "    #randomly showing an image\n",
    "    if(X == -1):\n",
    "        ind = np.random.randint(low=0, high=2000)\n",
    "        X = testX[ind,:]\n",
    "    print(testX.shape)\n",
    "    print(trainX.shape)\n",
    "    plt.figure()\n",
    "    plt.gray()\n",
    "    X_show = X.reshape(NUM_CHANNELS, IMG_SIZE, IMG_SIZE).transpose(1, 2, 0)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(X_show)\n",
    "    plt.savefig('./drive/My Drive/Colab Notebooks/a.1_b'+key+'1.png')\n",
    "    \n",
    "    h_conv1_, h_pool1_, h_conv2_, h_pool2_ = sess.run([conv_1, pool_1, conv_2, pool_2], {x: X_show.reshape(1, 3072)})\n",
    "    \n",
    "    print(h_conv1_.shape)\n",
    "    pylab.figure() \n",
    "    pylab.gray()\n",
    "    h_conv1_ = np.array(h_conv1_)\n",
    "    for i in range(c1_num_feature_map):\n",
    "      pylab.subplot(5, 10, i+1); pylab.axis('off'); pylab.imshow(h_conv1_[0,:,:,i])\n",
    "      pylab.savefig('./drive/My Drive/Colab Notebooks/a.1_b'+key+'2.png')\n",
    "    \n",
    "    print(h_pool1_.shape)\n",
    "    pylab.figure()\n",
    "    pylab.gray()\n",
    "    h_pool1_ = np.array(h_pool1_)\n",
    "    for i in range(c1_num_feature_map):\n",
    "        pylab.subplot(5, 10, i+1); pylab.axis('off'); pylab.imshow(h_pool1_[0,:,:,i])\n",
    "    pylab.savefig('./drive/My Drive/Colab Notebooks/a.1_b'+key+'3.png')\n",
    "    \n",
    "    print(h_conv2_.shape)\n",
    "    pylab.figure()\n",
    "    pylab.gray()\n",
    "    h_conv2_ = np.array(h_conv2_)\n",
    "    for i in range(c2_num_feature_map):\n",
    "        pylab.subplot(6, 10, i+1); pylab.axis('off'); pylab.imshow(h_conv2_[0,:,:,i])\n",
    "    pylab.savefig('./drive/My Drive/Colab Notebooks/a.1_b'+key+'4.png')\n",
    "\n",
    "    print(h_pool2_.shape)\n",
    "    pylab.figure()\n",
    "    pylab.gray()\n",
    "    h_pool2_ = np.array(h_pool2_)\n",
    "    for i in range(c2_num_feature_map):\n",
    "        pylab.subplot(6, 10, i+1); pylab.axis('off'); pylab.imshow(h_pool2_[0,:,:,i])\n",
    "    pylab.savefig('./drive/My Drive/Colab Notebooks/a.1_b'+key+'5.png')\n",
    "    pylab.show()\n",
    "    \n",
    "def q1():\n",
    "    global trainX, trainY, testX, testY, x, y_, logits, loss, train_step, W1, b1, conv_1, pool_1, W2, b2, conv_2, pool_2, W3, b3\n",
    "\n",
    "    train_acc, test_acc, loss_acc = batch_train(True)\n",
    "    \n",
    "    printCombinedGraph([loss_acc, test_acc], epochs, [\"loss\", \"test accuracy\"], \"accuracy / cost\", epochs, \"./drive/My Drive/Colab Notebooks/q1_accuracies.png\")\n",
    "    \n",
    "    printSingleGraph(train_acc, epochs, \"epochs\", \"train acc\", \"q1_b_train_acc\")\n",
    "    \n",
    "    printSingleGraph(test_acc, epochs, \"epochs\", \"test acc\", \"q1_b_test_acc\")\n",
    "    \n",
    "    printSingleGraph(loss_acc, epochs, \"epochs\", \"loss acc\", \"q1_b_loss_acc\")\n",
    "\n",
    "def q2():\n",
    "    global c1_num_feature_map, c2_num_feature_map, trainX, trainY, testX, testY, x, y_, logits, loss, train_step, W1, b1, conv_1, pool_1, W2, b2, conv_2, pool_2, W3, b3\n",
    "    \n",
    "    train_acc = []\n",
    "    test_acc = []\n",
    "    avg_test_acc = []\n",
    "    avg_test_acc_lbl = []\n",
    "    c1_train_acc = []\n",
    "    c1_test_acc = []\n",
    "    for c1_num_map in range(feature_map_start, feature_map_end, feature_map_increment):\n",
    "      print(\"C1 num feature map:\")\n",
    "      print(c1_num_map)\n",
    "      c1_num_feature_map = c1_num_map\n",
    "      train_acc = []\n",
    "      test_acc = []\n",
    "      for c2_num_map in range(feature_map_start_1, feature_map_end_1, feature_map_increment_1):\n",
    "        print(c2_num_map)\n",
    "        c2_num_feature_map = c2_num_map\n",
    "        \n",
    "        train_, test_ = cv_train()\n",
    "        \n",
    "        avg_test_acc.append( sum(test_)/epochs )\n",
    "        avg_test_acc_lbl.append(\"c1: \" + str(c1_num_map) +\" & c2: \"+ str(c2_num_map))\n",
    "    \n",
    "        train_acc.append(train_)\n",
    "        test_acc.append(test_)\n",
    "        \n",
    "      c1_train_acc.append(train_acc)\n",
    "      c1_test_acc.append(test_acc)\n",
    "      \n",
    "    test_key = 10\n",
    "    for c1_acc in c1_test_acc:\n",
    "      print(\"Test Acc for feature map of \" + str(test_key) + \" in convo 1\" )\n",
    "      print(c1_acc)\n",
    "      printCombinedGraph(c1_acc, epochs, [\"10\", \"20\", \"30\", \"40\", \"50\", \"60\", \"70\", \"80\", \"90\", \"100\"], \"accuracy\", \"num feature maps\", \"./drive/My Drive/Colab Notebooks/q2_test_\"+ str(test_key) +\"_.png\")\n",
    "      test_key += 10\n",
    "\n",
    "    train_key = 10\n",
    "    for c1_acc in c1_train_acc:\n",
    "      print(\"Test Acc for feature map of \" + str(train_key) + \" in convo 2\" )\n",
    "      printCombinedGraph(c1_acc, epochs, [\"10\", \"20\", \"30\", \"40\", \"50\", \"60\", \"70\", \"80\", \"90\", \"100\"], \"accuracy\", \"num feature maps\", \"./drive/My Drive/Colab Notebooks/q2_train_\"+ str(train_key) +\"_.png\")\n",
    "      train_key += 10\n",
    "      \n",
    "    #find highest\n",
    "    i = 0\n",
    "    highest = 0\n",
    "    highest_i = 0\n",
    "    for avg in avg_test_acc:\n",
    "      if avg > highest:\n",
    "        highest = avg\n",
    "        highest_i = i\n",
    "      i+=1\n",
    "    \n",
    "    print(avg_test_acc_lbl)\n",
    "    print(avg_test_acc)\n",
    "    print(\"best acc is :\" + avg_test_acc_lbl[highest_i])\n",
    "    print(\"highest avg is \" + str(highest))\n",
    "      \n",
    "\n",
    "def q3(c1_optimal_num_feature_map, c2_optimal_num_of_feature_map):\n",
    "  global c1_num_feature_map, is_momentum, is_rmsprop_learning_algo, is_adam_optimizer, is_dropout_layer,  c2_num_feature_map, trainX, trainY, testX, testY, x, y_, logits, loss, train_step, W1, b1, conv_1, pool_1, W2, b2, conv_2, pool_2, W3, b3\n",
    "  \n",
    "  c1_num_feature_map = c1_optimal_num_feature_map\n",
    "  \n",
    "  c2_num_feature_map = c2_optimal_num_of_feature_map\n",
    "  \n",
    "  train_acc = []\n",
    "  test_acc = []\n",
    "  loss_acc = []\n",
    "  plot_label = [\"Momentum 0.1\", \"RMSProp algo\", \"Adam Optimizer\", \"Dropout\"]\n",
    "\n",
    "  #a) add momentum\n",
    "  print(\"3d) Adding momentum 0.1...\") \n",
    "  is_momentum = True\n",
    "  \n",
    "  train_, test_, loss_ = batch_train()\n",
    "  \n",
    "  train_acc.append(train_)\n",
    "  test_acc.append(test_)\n",
    "  loss_acc.append(loss_)\n",
    "  #b) Using RMSProp algorithm for learning\n",
    "  print(\"3d) Using RMSProp learning algo...\")\n",
    "  is_momentum = False\n",
    "  is_rmsprop_learning_algo = True\n",
    "  \n",
    "  train_, test_, loss_ = batch_train()\n",
    "  \n",
    "  train_acc.append(train_)\n",
    "  test_acc.append(test_)\n",
    "  loss_acc.append(loss_)\n",
    "\n",
    "  #c) Using Adam optimizer for learning\n",
    "  print(\"3d) Using Adam Optimizer...\")\n",
    "  is_rmsprop_learning_algo = False\n",
    "  is_adam_optimizer = True\n",
    "  \n",
    "  train_, test_, loss_ = batch_train()\n",
    "  \n",
    "  train_acc.append(train_)\n",
    "  test_acc.append(test_)\n",
    "  loss_acc.append(loss_)\n",
    "\n",
    "  #d) Adding dropout to the layers\n",
    "  print(\"3d) Adding Dropout layer...\")\n",
    "  is_dropout_layer = True\n",
    "  is_adam_optimizer = False\n",
    "  \n",
    "  train_, test_, loss_ = batch_train()\n",
    "  \n",
    "  train_acc.append(train_)\n",
    "  test_acc.append(test_)\n",
    "  loss_acc.append(loss_)\n",
    "\n",
    "  #plot result\n",
    "  \n",
    "  printCombinedGraph(train_acc, epochs, plot_label, \"accuracy\", \"acc for \", \"./drive/My Drive/Colab Notebooks/q3_train_.png\")\n",
    "  printCombinedGraph(test_acc, epochs, plot_label, \"accuracy\", \"acc for \", \"./drive/My Drive/Colab Notebooks/q3_test_.png\")\n",
    "  printCombinedGraph(loss_acc, epochs, plot_label, \"loss\", \"loss for \", \"./drive/My Drive/Colab Notebooks/q3_loss_.png\")\n",
    "\n",
    "  \n",
    "if __name__ == '__main__':\n",
    "  #q1()\n",
    "  q2()\n",
    "  #q3(50, 60) #opitmal c1 feature map, optimal c2 feature map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
